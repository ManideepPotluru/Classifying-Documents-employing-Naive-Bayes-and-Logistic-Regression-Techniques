{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset = pd.read_csv(\"training.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_dataset =  original_dataset.copy()\n",
    "\n",
    "training_data, testing_data = train_test_split(dup_dataset, test_size = .2, random_state = 42)\n",
    "\n",
    "possible_classes = training_data[training_data.columns[-1]].unique()\n",
    "no_words = training_data.shape[1] -2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_classes = len(possible_classes) #number of classes\n",
    "\n",
    "d_number = training_data.shape[0] \n",
    "\n",
    "required_list = list(training_data[training_data.columns[-1]]) \n",
    "\n",
    "value_d= np.zeros((no_classes, d_number)) #matrix filled with zero's\n",
    "\n",
    "count = 0\n",
    "for rl in required_list:\n",
    "    # for class label on the count, set index = 1\n",
    "    value_d[rl-1, count] = 1\n",
    "    count += 1\n",
    "\n",
    "value_d = scipy.sparse.csr_matrix(value_d) #sparse matrix\n",
    "\n",
    "#dropping columns that are not necessary\n",
    "training_data = training_data.drop('1', 1)\n",
    "\n",
    "training_data = training_data.drop('14', 1)\n",
    "#creating sparse with training data\n",
    "actual_req_train = scipy.sparse.csr_matrix(training_data)\n",
    "#normalising the actual_req_train\n",
    "add_c = np.array(actual_req_train.sum(axis=0))[0,:]\n",
    "add_c[add_c==0]=1\n",
    "actual_req_train /= add_c\n",
    "\n",
    "nor_actual_req_train = actual_req_train\n",
    "   \n",
    "weights = scipy.sparse.csr_matrix(np.zeros((no_classes, no_words), dtype=np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists of learning_rate and penalty_terms for tuning logistic regression.\n",
    "list_lr = [.0001, .001, .0025, .0050, .0075, .01, .1]\n",
    "list_p = [.0001, .001, .0025, .0050, .0075, .01, .1]\n",
    "\n",
    "\n",
    "updates = 100 # Number of weight updates in logistic regression\n",
    "lr = .01 # Learning or eta term\n",
    "pr = .005 #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(updates):\n",
    "        # matrix of probabilities, P( Y | W, X) ~ exp(W * X^T)\n",
    "        Z_term = np.expm1(weights.dot(actual_req_train.transpose()))\n",
    "        #normalize Z\n",
    "        add_c = np.array(Z_term.sum(axis=0))[0,:] # column vector\n",
    "        add_c[add_c==0]=1\n",
    "        Z_term /= add_c\n",
    "        # = Z / Z.sum(axis=0)\n",
    "        # gradient w.r.t. Weights with regularization\n",
    "        gradient = ((value_d - Z_term) * actual_req_train) - (pr * weights)\n",
    "        # learning rule\n",
    "        weights = weights + (lr * gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.525\n",
      "[[ 7  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0]\n",
      " [ 1  1  0  0  0  3  0  0  0  0  0  1  0  0  0  2  0  2  0  0]\n",
      " [ 1  0  5  1  0  2  0  0  0  0  0  0  0  1  0  3  0  0  0  0]\n",
      " [ 0  0  0  9  0  1  0  0  0  0  0  0  0  0  0  1  0  0  0  0]\n",
      " [ 1  0  0  2  2  0  0  0  0  0  0  0  0  0  0  3  0  0  0  0]\n",
      " [ 0  0  0  0  1  4  0  0  0  0  0  0  0  0  0  2  0  0  0  0]\n",
      " [ 1  1  0  0  0  0  1  0  0  1  0  0  2  0  0  4  1  0  1  0]\n",
      " [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  2  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  1  6  0  0  0  0  0  0  3  0  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  8  0  0  1  0  0  3  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  8  0  0  0  0  0  0  2  1  0]\n",
      " [ 1  0  0  0  0  0  0  0  0  0  0  7  0  0  0  3  0  0  1  0]\n",
      " [ 0  0  0  1  0  0  0  0  0  0  0  0  5  0  0  2  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  4  0  6  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  1  0  1  4  0]\n",
      " [ 2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  9  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  5  8  0  2  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0  5  0  0]\n",
      " [ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  1  3  0]\n",
      " [ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  4  0  0  0  1]]\n"
     ]
    }
   ],
   "source": [
    "train_weights =weights\n",
    "# 20% of training set for calculating training accuracy\n",
    "\n",
    "test_classes = list(testing_data[testing_data.columns[-1]])\n",
    "testing_data_copy = testing_data.copy()\n",
    "\n",
    "testing_data = testing_data.drop('1', 1)\n",
    "testing_data = testing_data.drop('14', 1)\n",
    "\n",
    "values_guessed = np.expm1(train_weights.dot(testing_data.transpose()))\n",
    "# take maximum and get index for every example\n",
    "maximum_index = values_guessed.argmax(axis=0).ravel().tolist()\n",
    "result = []\n",
    "for i in range(values_guessed.shape[1]):\n",
    "    result.append(maximum_index[0][i] + 1)\n",
    "    \n",
    "values_pred = result\n",
    "predictions_array = np.array(values_pred)\n",
    "testing_data_results = np.array(testing_data_copy.iloc[:,-1])\n",
    "\n",
    "\n",
    "accuracy_score = accuracy_score(testing_data_results, predictions_array)\n",
    "print(accuracy_score)  # Training accuracy\n",
    "#drawing the confusion matrix for fixed lr,pr values.\n",
    "confusion_matrix = confusion_matrix(testing_data_results,predictions_array)\n",
    "print(confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the testing dataset\n",
    "actual_testing_data = pd.read_csv(\"testing.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_copy = actual_testing_data.copy()\n",
    "testing_copy = testing_copy.drop(0, 1)\n",
    "\n",
    "predicted = np.expm1(train_weights.dot(testing_copy.transpose()))\n",
    "# take maximum and get index for every example\n",
    "maximum_index = predicted.argmax(axis=0).ravel().tolist()\n",
    "result = []\n",
    "for i in range(predicted.shape[1]):\n",
    "    result.append(maximum_index[0][i] + 1)\n",
    "    \n",
    "final_predictions = result\n",
    "\n",
    "# Saving the predictions to the csv file\n",
    "result_frame = pd.DataFrame(final_predictions)\n",
    "result_frame.to_csv(\"predicted_result_logistic_mani.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_values = []\n",
    "for lr in list_lr:\n",
    "    for pr in list_p:\n",
    "        dup_dataset =  original_dataset.copy()\n",
    "\n",
    "        training_data, testing_data = train_test_split(dup_dataset, test_size = .2, random_state = 42)\n",
    "\n",
    "        possible_classes = training_data[training_data.columns[-1]].unique()\n",
    "        no_words = training_data.shape[1] -2 \n",
    "        \n",
    "        training_copy = training_data.copy()\n",
    "        test_copy = testing_data.copy()\n",
    "        possible_classes = training_data[training_data.columns[-1]].unique()\n",
    "        no_words = training_data.shape[1] -2\n",
    "        no_classes = len(possible_classes)\n",
    "        d_number = training_data.shape[0]\n",
    "\n",
    "        required_list = list(training_data[training_data.columns[-1]])\n",
    "        value_d= np.zeros((no_classes, d_number))\n",
    "\n",
    "        count = 0\n",
    "        # go through each examples classification and \n",
    "        # index into the matrix delta and set that indice to 1\n",
    "        # need to subtract 1 from the label because labels are 1-indexed\n",
    "        for rl in required_list:\n",
    "            # for class label on the count, set index = 1\n",
    "            value_d[rl-1, count] = 1\n",
    "            count += 1\n",
    "\n",
    "        value_d = scipy.sparse.csr_matrix(delta)\n",
    "\n",
    "        training_data = training_data.drop('1', 1)\n",
    "\n",
    "        training_data = training_data.drop('14', 1)\n",
    "\n",
    "        actual_req_train = scipy.sparse.csr_matrix(training_data)\n",
    "        #\n",
    "        add_c = np.array(actual_req_train.sum(axis=0))[0,:] # column vector\n",
    "        add_c[add_c==0]=1\n",
    "        actual_req_train /= add_c\n",
    "\n",
    "        actual_req_train = actual_req_train\n",
    "\n",
    "        #    \n",
    "        weights = scipy.sparse.csr_matrix(np.zeros((no_classes, no_words), dtype=np.float64))\n",
    "\n",
    "        print (weights.shape, actual_req_train.shape)\n",
    "        training_accuracies = []\n",
    "        for i in range(updates):\n",
    "            # matrix of probabilities, P( Y | W, X) ~ exp(W * X^T)\n",
    "            Z_term = np.expm1(weights.dot(actual_req_train.transpose()))\n",
    "            #normalize Z\n",
    "            add_c = np.array(Z_term.sum(axis=0))[0,:] # column vector\n",
    "            add_c[add_c==0]=1\n",
    "            Z_term /= add_c\n",
    "            # = Z / Z.sum(axis=0)\n",
    "            # gradient w.r.t. Weights with regularization\n",
    "            gradient = ((value_d - Z_term) * actual_req_train) - (pr * weights)\n",
    "            # learning rule\n",
    "            weights = weights + (lr * gradient)\n",
    "                \n",
    "        train_weights =weights\n",
    "        # 20% of training set for calculating training accuracy\n",
    "\n",
    "        test_classes = list(testing_data[testing_data.columns[-1]])\n",
    "\n",
    "        testing_data = testing_data.drop('1', 1)\n",
    "        testing_data = testing_data.drop('14', 1)\n",
    "\n",
    "        values_guessed = np.expm1(train_weights.dot(testing_data.transpose()))\n",
    "        # take maximum and get index for every example\n",
    "        maximum_index = predicted.argmax(axis=0).ravel().tolist()\n",
    "        result = []\n",
    "        for i in range(predicted.shape[1]):\n",
    "            result.append(maximum_index[0][i] + 1)\n",
    "\n",
    "        values_pred = result\n",
    "            \n",
    "        count = 0\n",
    "        for i in range(len(test_classes)):\n",
    "            if test_classes[i] == values_pred[i]: # Comparing predicted class with original class\n",
    "                count +=1\n",
    "\n",
    "        training_accuracy = float(count) / len(test_classes)\n",
    "        training_accuracies.append(training_accuracy)\n",
    "\n",
    "        optimal_acc = training_accuracies\n",
    "        print ( lr, pr, max(optimal_acc))\n",
    "        tuning_values.append((lr, pr, optimal_acc))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
